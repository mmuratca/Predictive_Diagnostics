"""
Hardware Diagnostics Health Score Prediction
--------------------------------------------
Adapted from Turkuoise_Model with secure terminology and improvements:
- Leave-One-Device-Out validation
- Feature encoding
- SHAP explainability
- Model architecture preserved (LSTM + Dense)

Author: [REDACTED]
"""

# -*- coding: utf-8 -*-
"""Turkuoise_Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ReKKE6OHf6IFkxJLprgd10UsceErhav
"""

# Commented out IPython magic to ensure Python compatibility.
#!pip install tf-nightly-gpu-2.0-preview
#!pip show tensorflow
#!pip install --upgrade tensorflow
import tensorflow as tf
print(tf.__version__)
from tensorflow import keras
import pandas as pd
import numpy as np
import seaborn as sns; sns.set()
import math
# %matplotlib inline
import matplotlib as plt


# !pip install talos
# import talos as ta
#!pip install -q tf-nightly-2.0-preview
from tensorflow import summary
# %load_ext tensorboard.notebook
import datetime

!pip install shap
import shap
shap.initjs()

#!git clone --recursive https://github.com/alshedivat/kgp
#!cd kgp

from matplotlib import pyplot
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.utils.vis_utils import plot_model
from mpl_toolkits.mplot3d import Axes3D
from keras.layers.advanced_activations import LeakyReLU
from scipy import stats

#from kgp.layers import GP
#from kgp.models import Model
#from kgp.losses import gen_gp_loss
#from talos.model.normalizers import lr_normalizer

# Clear any logs from previous runs
#!rm -rf ./logs/

import tensorflow as tf
print(tf.__version__)

#Data Read
from google.colab import files
uploaded = files.upload()

#DATA PREP (TIMESERIES DATA)
Sys1Health = pd.read_csv("/content/System1SveCr.csv")
Sys1Health = Sys1Health.fillna(0)
Sys1Health = Sys1Health.drop(["week", "Grand Total" , "Project"],axis=1)
Sys4Health = pd.read_csv("/content/System2SveCr.csv")
Sys4Health = Sys4Health.fillna(0)
Sys4Health = Sys4Health.drop(["week", "Project"],axis=1)
Sys3Health = pd.read_csv("/content/System2SveCr.csv")
Sys3Health = Sys3Health.fillna(0)
Sys3Health = Sys3Health.drop(["week", "Project"],axis=1)
Sys2Health = pd.read_csv("/content/System2SveCr.csv")
Sys2Health = Sys2Health.fillna(0)
Sys2Health = Sys2Health.drop(["week", "Project"],axis=1)

#print(Sys1Health.head())
print(np.var(Sys1Health))
pd.set_option('display.width', 200)
pd.set_option('precision', 3)
print(Sys1Health.describe())



Sys1Health.plot()
pyplot.title ("weekly error")
pyplot.xlabel("weeks")
pyplot.ylabel("error count")
#sns.pairplot(Sys1Health, palette='RdBu_r', size=1.5)






NzMx = Sys1Health.values
input_Sys1Health=np.delete(NzMx,(NzMx.shape[0]-1),axis=0)
input_Sys1Health=np.vstack([np.zeros(shape=[input_Sys1Health.shape[1],]),input_Sys1Health])


NpMx=Sys4Health.values
input_Sys4Health=np.delete(NzMx,(NpMx.shape[0]-1),axis=0)
input_Sys4Health=np.vstack([np.zeros(shape=[input_Sys4Health.shape[1],]),input_Sys4Health])

HnMx=Sys3Health.values
input_Sys3Health=np.delete(HnMx,(HnMx.shape[0]-1),axis=0)
input_Sys3Health=np.vstack([np.zeros(shape=[input_Sys3Health.shape[1],]),input_Sys3Health])

#KnMx=Sys3Health.values
#input_Sys2Health=np.delete(KnMx,(KnMx.shape[0]-1),axis=0)
#input_Sys2Health=np.vstack([np.zeros(shape=[input_Sys2Health.shape[1],]),input_Sys2Health])

#DATA PREP (FEATURE MATRIX)
#data-input-matrix
DKLOC = pd.read_csv("DeltaKLOC.csv")
DKLOC = DKLOC.drop(["Core"],axis=1)
DKLOC = DKLOC.apply(pd.to_numeric)
print(DKLOC)

#normalize data
#DKLOC=(DKLOC-min(DKLOC.min()))/(max(DKLOC.max())-min(DKLOC.min()))
DKLOC = (DKLOC/DKLOC.max())
print (DKLOC)

DKLOC_test = DKLOC['System2']
DKLOC_val = DKLOC['System2']

DKLOC_train=DKLOC.drop(['System2','System2'],axis=1)
print(DKLOC_train)

DKLOC_train=DKLOC_train.values
DKLOC_test=np.reshape(DKLOC_test.values,[-1,1])
DKLOC_val=np.reshape(DKLOC_val.values,[-1,1])

DKLOC_train=np.stack([DKLOC_train.T.tolist()]*91,axis=1)
DKLOC_test=np.stack([DKLOC_test.T.tolist()]*91,axis=1)
DKLOC_val=np.stack([DKLOC_val.T.tolist()]*91,axis=1)

#print(DKLOC_test[0,:,:])
print(DKLOC_test.shape)


#DeltaKLOC=DKLOC.values

input_data=np.stack([input_Sys1Health,input_Sys4Health])
val_data=np.reshape(input_Sys3Health,[1,91,14])
#test_data=np.reshape(input_Sys2Health,[1,91,14])
print(input_data.shape)
print(val_data.shape)

# LSTM https://chrisalbon.com/deep_learning/keras/lstm_recurrent_neural_network/

# Multiple Inputs
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers.merge import concatenate
from keras.layers.recurrent import LSTM
from keras.layers import Reshape

#model
#model = Sequential()
#model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))
#model.add(LSTM(hidden_size, return_sequences=True))
#model.add(LSTM(hidden_size, return_sequences=True))

# Define
n_project=3
timesteps=91
data_dim=14
hidden_dim=28  ##oyna
MLP_hidden_dim=8  #oyna
hidden_dim3=9

# Model
#activations relu elu selu sigmoid tanh softplus softsign hard_sigmoid exponential linear
#kernel_initializer='uniform'


time_input= Input(shape=(timesteps,data_dim))           #inputs dimension
DKLOC_input= Input(shape=(timesteps,data_dim))
print (time_input)
print (DKLOC_input)

LSTM_outs=LSTM(hidden_dim, unroll= True, return_sequences=True)(time_input)
print (LSTM_outs) #Give input to the Function

MLP_inputs=concatenate([LSTM_outs,DKLOC_input])
print (MLP_inputs)

MLP_outs= Dense(MLP_hidden_dim, activation='relu')(MLP_inputs)
print (MLP_outs)

MLP_outs= Dense(MLP_hidden_dim, activation='relu')(MLP_outs)
print (MLP_outs)

#MLP_outs= Dense(hidden_dim3, activation='relu')(MLP_outs)
#print (MLP_outs)

#MLP_outs= Dense(hidden_dimC, activation='relu')(MLP_outs)
#model.add(Dense(hidden_dimC, activation='relu'))
##2 MLP_outs= Dense(MLP_hidden_dim, activation='relu')(MLP_outs)#

outs= Dense(data_dim, activation="linear")(MLP_outs)
print (outs)

#define output

truth = np.stack([NzMx, NpMx], axis=0)
val_truth = np.reshape(HnMx,[1,91,14])

#x_train = [input_data, DKLOC_train]
#y_val = val_truth

print(truth.shape)
print(val_truth)

#set randomness
from numpy.random import seed
seed(0)
from tensorflow import set_random_seed
set_random_seed(2)

#OPTIMIZATION #COMPILE #FIT
model = Model(inputs=[time_input,DKLOC_input], outputs=[outs])
print (outs)

#optimizers = adam adamax rmsprop nadam adadelta adagrad sgd

model.compile(loss='mse', optimizer="nadam", metrics=['mse', 'mae', 'mape', 'cosine'])

history = model.fit(x=[input_data, DKLOC_train] , y= truth, batch_size=1, epochs=100, verbose=2)

#VISUAL
print(model.summary())
pyplot.plot(history.history['mean_squared_error'])
#pyplot.plot(history.history['mean_absolute_error'])
#pyplot.plot(history.history['mean_absolute_percentage_error'])
#pyplot.plot(history.history['cosine_proximity'])
pyplot.title ("Metrics")
pyplot.xlabel("epochs")
pyplot.ylabel("loss")
pyplot.legend(['Training Loss', 'Test Loss'])
pyplot.show()
print(history.history.keys())


#score = model.evaluate(x, y, verbose=0)
#print('Test loss:', score[0])
#print('Test accuracy:', score[1])

#PREDICTION
val_prediction = model.predict([val_data,DKLOC_val])
val_error = np.mean((val_prediction-val_truth)**2)
print('validation error is {}'.format(val_error))


#https://nbviewer.jupyter.org/github/autonomio/talos/blob/master/examples/Hyperparameter%20Optimization%20with%20Keras%20for%20the%20Iris%20Prediction.ipynb

# def iris_model(x_train, y_train, x_val, y_val, params):

#     model = Sequential()
#     model.add(Dense(params['first_neuron'],
#                     input_dim=x_train.shape[1],
#                     activation='relu'))

#     model.add(Dropout(params['dropout']))
#     model.add(Dense(y_train.shape[1],
#                     activation=params['last_activation']))

#     model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),
#                   loss=params['loss'],
#                   metrics=['acc'])

#     out = model.fit(x_train, y_train,
#                     batch_size=params['batch_size'],
#                     epochs=params['epochs'],
#                     verbose=0,
#                     validation_data=[x_val, y_val])

#     return out, model

# p = {'lr': (0.1, 10, 10),
#      'first_neuron':[4, 8, 16, 32, 64, 128],
#      'batch_size': [2, 3, 4],
#      'epochs': [200],
#      'dropout': (0, 0.40, 10),
#      'optimizer': [Adam, Nadam],
#      'loss': ['categorical_crossentropy'],
#      'last_activation': ['softmax'],
#      'weight_regulizer': [None]}

"""# New Section"""

#http://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/
#https://medium.com/@tommytao_54597/use-tensorboard-in-google-colab-16b4bb9812a6
#https://www.codementor.io/blog/tensorboard-integration-5fh168wqvi
#http://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/
#https://stackoverflow.com/questions/45309153/structure-a-keras-tensorboard-graph

#https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
#https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/

#model
model = keras.Sequential()
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(100, activation='softmax'))

#placeholder input output

#initialize weights

#histogram for weights

#cost function

#compile
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

#train
train_dataset = dataset.sample(frac=0.8,random_state=0)
test_dataset = dataset.drop(train_dataset.index)

#fit
model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 256, verbose=2, epochs=5)



#accuracy
score = model.evaluate(x_test, y_test, verbose=0)
print('Test accuarcy: {:0.2f}%'.format(score[1] * 100))

#session



#prediction

#improve accuracy



# tensorboardu ac
$ tensorboard --logdir=/tmp/Turkoise_Model
browserda: localhost:6006

#MULTI INPUT LST
#https://stackoverflow.com/questions/42532386/how-to-work-with-multiple-inputs-for-lstm-in-keras
#https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output
#https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models

#File Upload

#Now, load the contents of the file into a Pandas DataFrame using the following:
#import pandas as pd
#import io
#df = pd.read_csv(io.StringIO(uploaded['XXXXX.csv'].decode('utf-8')))
#print(df)

#alternative data loading
#dataset = pd.read_csv("gdrive/My Drive/Colab Notebooks/XXXXg.csv")
#dataset.head()

!npm init -y
!npm install ijavascript zeromq

!npm install -g --unsafe-perm ijavascript zeromq

tensorboard --logdir=path/to/log-directory

def load_ipython_extension(ipython):
  from tensorboard import notebook  # pylint: disable=g-import-not-at-top
  return notebook.load_ipython_extension(ipython)

# https://colab.research.google.com/drive/1ndNCQmID2x7Gk_gNGer2k4yz4EJ1ZuG0#scrollTo=4qudkDxmrzSe
# https://colab.research.google.com/github/irhallac/deep_learning_examples/blob/master/CIFAR_TENSORBOARD_COLAB.ipynb#scrollTo=XBOO22wln6g4

# Run tensorboard with the logdir
LOG_DIR = '/tmp/log'
get_ipython().system_raw(
    'tensorboard --logdir="example_4" --host 0.0.0.0 --port 6006 &'
    .format(LOG_DIR)
)



# check whether the tensorboard has started successfully
! curl http://localhost:6006

# Get the installation zip of ngrok which creates  tunnel from public ip to the private locahost
! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1

# Verify the installation files
! ls

# Unzip the installation
! unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1

# Run Ngrok with port 6006
get_ipython().system_raw('./ngrok http 6006 &')

# get the public ip to view tensorboard webui
! curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

# Command to check
! ps -eaf | grep 6006

# maintainance commands, to kill extra or wrong sessions of tensorboard
! kill -9 828

#set randomness
from numpy.random import seed
seed(0)
from tensorflow import set_random_seed
set_random_seed(2)

#OPTIMIZATION #COMPILE #FIT
model = Model(inputs=[time_input,DKLOC_input], outputs=[outs])
print (outs)

#optimizers = adam adamax rmsprop nadam adadelta adagrad sgd

model.compile(loss='mse', optimizer="nadam", metrics=['mse', 'mae', 'mape', 'cosine'])
history = model.fit(x=[input_data, DKLOC_train] , y= truth, batch_size=1, epochs=50, verbose=2)
pyplot.plot(history.history['mean_squared_error'])
#pyplot.plot(history.history['mean_absolute_error'])
#pyplot.plot(history.history['mean_absolute_percentage_error'])
#pyplot.plot(history.history['cosine_proximity'])

#VISUAL
pyplot.title ("Metrics")
pyplot.xlabel("epochs")
pyplot.ylabel("loss")
pyplot.legend(['Training Loss', 'Test Loss'])
pyplot.show()
print(history.history.keys())

#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)
#score = model.evaluate(x, y, verbose=0)
#print('Test loss:', score[0])
#print('Test accuracy:', score[1])

#PREDICTION
val_prediction = model.predict([val_data,DKLOC_val])
val_error = np.mean((val_prediction-val_truth)**2)
print('validation error is {}'.format(val_error))

# === Injected Hardware Diagnostics CV and SHAP Logic ===

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
import shap
import os

# Load and prepare dataset
df = pd.read_excel("hardware_diagnostics_data.xlsx", sheet_name='CleanForSystem1')
df = df.dropna()

# Rename columns if original Excel headers are raw
df.columns = [col.strip() for col in df.columns]

# Encode categorical features
df['CORE_TYPE'] = LabelEncoder().fit_transform(df['CORE_TYPE'])

# Define target and features
target = 'HEALTH_SCORE'
features = ['CORE_TYPE', 'SYSTEM_SIZE', 'SYSTEM_CHANGE', '%_SYSTEM_CHANGE',
            'LOGIC_ARCH_CHANGE', 'AI_ENGINE_VARIANT', 'FAB_PROCESS_VARIANT',
            'AI_MODEL_DEPTH', 'BATCH_SEQ', 'INSTANCE_ID', 'INSTRUCTION_DELTA']

# Leave-One-Device-Out Cross-Validation
unique_devices = df['DEVICE_ID'].unique()
results = []

def build_model(input_shape):
    from keras.models import Sequential
    from keras.layers import Dense, Dropout, LSTM, Flatten

    model = Sequential()
    model.add(LSTM(64, input_shape=(1, input_shape), return_sequences=True))
    model.add(Flatten())
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1))  # Regression output
    model.compile(optimizer='adam', loss='mae', metrics=['mae'])
    return model

for dev_id in unique_devices:
    train = df[df['DEVICE_ID'] != dev_id]
    test = df[df['DEVICE_ID'] == dev_id]

    X_train, y_train = train[features], train[target]
    X_test, y_test = test[features], test[target]

    # Reshape for LSTM [samples, timesteps, features]
    X_train = np.expand_dims(X_train.values, axis=1)
    X_test = np.expand_dims(X_test.values, axis=1)

    model = build_model(X_train.shape[2])
    model.fit(X_train, y_train, epochs=30, verbose=0)

    y_pred = model.predict(X_test).flatten()
    test['PREDICTED_HEALTH'] = y_pred
    test['ABS_ERROR'] = np.abs(y_pred - y_test.values)
    results.append(test)

    # SHAP explainability (per-device level)
    explainer = shap.Explainer(model.predict, X_train)
    shap_values = explainer(X_test)
    shap.summary_plot(shap_values, pd.DataFrame(X_test.squeeze(), columns=features))

# Save full predictions
final_results = pd.concat(results)
final_results.to_excel("System_Health_Diagnostics_Predictions.xlsx", index=False)
print("Saved predictions to System_Health_Diagnostics_Predictions.xlsx")
